<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Navigating the Non-Linear Path of Battery Life Prediction</title>
    <style>
        body {
            font-family: Georgia, 'Times New Roman', serif;
            line-height: 1.8;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 40px 20px 20px 20px;
            background: #fafafa;
        }

        .home-button {
            position: fixed;
            top: 20px;
            left: 20px;
            background: #333;
            color: white;
            text-decoration: none;
            padding: 8px 16px;
            border-radius: 4px;
            font-size: 0.9em;
            font-family: Georgia, serif;
            transition: background-color 0.3s ease;
        }

        .home-button:hover {
            background: #555;
            color: white;
            text-decoration: none;
        }

        footer {
            margin-top: 4em;
            padding: 2em 0 1em 0;
            border-top: 1px solid #ccc;
            text-align: center;
            font-size: 0.9em;
            color: #666;
        }

        h1 {
            font-size: 2.2em;
            text-align: center;
            margin-bottom: 0.5em;
            border-bottom: 2px solid #333;
            padding-bottom: 20px;
        }

        h2 {
            font-size: 1.5em;
            margin-top: 2em;
            margin-bottom: 1em;
            border-bottom: 1px solid #ccc;
            padding-bottom: 10px;
        }

        h3 {
            font-size: 1.2em;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
            font-style: italic;
        }

        p {
            margin-bottom: 1.2em;
            text-align: justify;
        }

        .abstract {
            font-style: italic;
            margin: 2em 0;
            padding: 20px;
            background: #f5f5f5;
            border-left: 4px solid #333;
        }

        .features-list {
            margin: 1em 0;
            padding-left: 2em;
        }

        .features-list li {
            margin-bottom: 0.5em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 2em 0;
            font-size: 0.9em;
        }

        th, td {
            border: 1px solid #ddd;
            padding: 8px 12px;
            text-align: left;
        }

        th {
            background-color: #f8f8f8;
            font-weight: bold;
        }

        .version-section {
            margin: 2em 0;
            padding: 1.5em 0;
        }

        .version-title {
            font-weight: bold;
            font-size: 1.1em;
            margin-bottom: 1em;
        }

        .findings {
            margin: 1em 0;
            padding: 15px;
            background: #f9f9f9;
            border: 1px solid #e0e0e0;
        }

        .findings h4 {
            margin-top: 0;
            margin-bottom: 0.8em;
            font-size: 1em;
            text-decoration: underline;
        }

        .findings ul {
            margin: 0;
            padding-left: 1.5em;
        }

        .findings li {
            margin-bottom: 0.5em;
        }

        @media (max-width: 600px) {
            body {
                padding: 20px 15px;
            }
            
            h1 {
                font-size: 1.8em;
            }
            
            table {
                font-size: 0.8em;
            }
        }
    </style>
</head>
<body>
    <a href="blog.html" class="home-button">← Home</a>
    
    <h1>Navigating the Non-Linear Path of Battery Life Prediction</h1>

    <div class="abstract">
        When I started this project, the core goal was straightforward: quantify how much extending a battery's lifespan is possible by reducing thermal stress through a supercapacitor-hybrid system. That fundamental objective still guides my work, but as any data scientist or engineer can attest, real progress rarely follows a straight line. It often emerges from a landscape of messy, poorly documented notebooks, abandoned scripts, and perplexing outliers that challenge every assumption.
    </div>

    <h2>Establishing a Foundation: The HNEI Dataset</h2>

    <p>To kick things off, the Hawaii Natural Energy Institute (HNEI) dataset was chosen as the starting point for model development. Compared to the NASA Battery Aging Dataset, HNEI offers a simpler, more structured entry point. It provides features for each cycle of 14 LG Chem 18650 lithium-ion batteries.</p>

    <p>These features include:</p>
    <ul class="features-list">
        <li><strong>Cycle:</strong> The cycle number</li>
        <li><strong>discharge_time:</strong> Total discharge duration per cycle</li>
        <li><strong>charge_time:</strong> Total charge duration</li>
        <li><strong>min_voltage and max_voltage:</strong> Minimum and maximum voltages during each cycle</li>
        <li><strong>min_temp and max_temp:</strong> Temperature extremes per cycle</li>
        <li><strong>avg_current:</strong> Current-related metrics (depending on the version)</li>
        <li><strong>RUL:</strong> The Remaining Useful Life label (measured in cycles to end-of-life)</li>
    </ul>

    <p>The structured nature of this dataset makes it exceptionally well-suited for traditional machine learning models (see <a href="ML.html">Traditional ML</a> ). These models excel with tabular, cycle-level data, enabling rapid experimentation with various feature combinations and modeling strategies.</p>

    <div class="findings" style="margin-top:1.5em;">
        <h4>Insight: Machine Learning and Deep Learning</h4>
        <ul>
            <li>
                Traditional ML methods (like linear regression or random forests) are much faster than deep learning for small-to-medium datasets (often seconds versus hours) since they use simpler math, run on CPUs, and don’t require massive data.
            </li>
            <li>
                For regression problems, starting with traditional ML is almost always best: these models are interpretable, data-efficient, and provide strong baselines quickly.
            </li>
            <li>
                Deep learning only becomes necessary if you have huge datasets, complex patterns (such as images or text), or if traditional methods plateau. This saves time, avoids overkill, and keeps things simple.
            </li>
        </ul>
    </div>

    <p>Working with the HNEI dataset at this initial stage serves two critical purposes:</p>

    <ol>
        <li><strong>Baseline Performance Benchmarking:</strong> Building models on a clean, standardized dataset allows for precise quantification of model quality using interpretable metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and R². These benchmarks will be crucial comparison points when progressing to more complex datasets, like NASA’s, which will demand significant additional preprocessing and sequence modeling.
        </li>
        <li>
            <strong>Controlled Environment for Model Exploration:</strong> The HNEI dataset provides a controlled setting to investigate model behavior and sensitivity. For instance, it allows for analysis of how performance changes with different train/test splitting strategies (e.g., by battery, cycle, or end-of-life). It also supports feature importance analysis, outlier detection, and error analysis with minimal preprocessing overhead. These insights are vital for building an intuitive understanding of what works (and what breaks) before transitioning to more sophisticated approaches like LSTMs or transformer-based networks.
        </li>
    </ol>
    <div class="findings">
        <h4>Insights: Key Metrics for Linear Regression</h4>
        <ul>
            <li>
                <strong>R² (R-squared):</strong> Measures how much variance your model explains (ranges from 0 to 1; higher values indicate a better fit).
            </li>
            <li>
                <strong>Mean Squared Error (MSE):</strong> Average of squared prediction errors (lower is better, but sensitive to outliers). As a rule of thumb, a "good" MSE is about 10% of your target variable’s variance in this case RUL has a variance of 103,961.
            </li>
            <li>
                <strong>Root Mean Squared Error (RMSE):</strong> Square root of MSE, expressed in the original units of the target variable (easier to interpret).
            </li>
            <li>
                <strong>Mean Absolute Error (MAE):</strong> Average of absolute errors (more robust to outliers, but does not indicate error direction).
            </li>
            <li>
                Use R² for overall fit, RMSE/MAE for error magnitude—prefer MAE if outliers distort results. Always check residual plots to spot hidden patterns.
            </li>
        </ul>
    </div>

    <h2>The Versions So Far—and What They've Taught Me</h2>

    <p>I've structured the project into several distinct experimental versions. This approach has been instrumental in surfacing meaningful insights about the data, the models, and the requisites for making robust Remaining Useful Life (RUL) predictions.</p>

    <div class="version-section">
        <div class="version-title">Version 1: Data Exploration and First Model Run [ <a href="https://colab.research.google.com/drive/1Ka1GBcjUoSjvlVJ44iDyVvkXWWWXE4mQ"> Notebook</a> ]</div>
        
        <p>The first version served primarily as a diagnostic tool—a rapid, exploratory sweep to confirm whether machine learning could effectively capture degradation patterns within the HNEI dataset. A comprehensive data inspection was conducted, confirming no missing values or duplicates. Early correlation analysis identified relationships between input features and the RUL target.</p>

        <p>Crucially, this phase highlighted a significant insight: certain columns, particularly the cycle index, were highly correlated with the target variable, introducing severe data leakage if included, as the results will show </p>

        <div class="findings">
            <h4>Key Findings:</h4>
            <ul>
                <li><strong>Data Integrity:</strong> No missing values or duplicate rows were found</li>
                <li><strong>Feature Correlation with RUL:</strong>
                    <ul>
                        <li>Cycle_Index: -0.9998 (Excluded due to data leakage)</li>
                        <li>Max. Voltage Discharge: +0.7828</li>
                        <li>Min. Voltage Charge: -0.7598</li>
                    </ul>
                </li>
                <li><strong>Cycle_Index</strong> was identified as a major data leakage source due to its near-perfect correlation with RUL</li>
                <li> Battery Id might also be a source of Data leakage </li>
            </ul>
        </div>

        <table>
            <thead>
                <tr>
                    <th>Model</th>
                    <th>MAE</th>
                    <th>MSE</th>
                    <th>RMSE</th>
                    <th>R²</th>
                    <th>MAPE</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Extra Trees Regressor</td>
                    <td>5.39</td>
                    <td>255.53</td>
                    <td>15.41</td>
                    <td>0.9976</td>
                    <td>7.22%</td>
                </tr>
                <tr>
                    <td>Ensemble of Top Models</td>
                    <td>6.69</td>
                    <td>261.10</td>
                    <td>15.61</td>
                    <td>0.9975</td>
                    <td>8.28%</td>
                </tr>
            </tbody>
        </table>

        <p>These results are unrealistically strong—likely due to data leakage. Without a proper train-test split, the model was evaluated on data it had already seen during training, artificially inflating performance. While the features may contain useful signals, true predictive power can only be validated with rigorous holdout testing.</p>
    </div>

    <div class="version-section">
        <div class="version-title">Version 2: Battery Split Validation [ <a href="https://colab.research.google.com/drive/1bsbEtcKzKqwT0zxOLXzAS9_iyrbrSMgi"> Notebook</a> ]</div>
        
        <p>Version 2 marked a significant shift in data approach. Instead of randomly sampling data points, the strategy involved withholding entire batteries from the training set. This battery-wise split more accurately simulates a real-world scenario: deploying a model on a battery it has never seen before.</p>

        <p>Consequently, performance dropped—precisely as expected. This revealed that earlier results were likely inflated by leakage between train and test data originating from the same battery, allowing the model to memorize rather than generalize.</p>

        <table>
            <thead>
                <tr>
                    <th>Condition</th>
                    <th>MAE</th>
                    <th>MSE</th>
                    <th>RMSE</th>
                    <th>R²</th>
                    <th>MAPE</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>With Outliers</td>
                    <td>8.8735</td>
                    <td>387.4732</td>
                    <td>19.4128</td>
                    <td>0.9962</td>
                    <td>0.1222</td>
                </tr>
                <tr>
                    <td>Without Battery_ID</td>
                    <td>26.9881</td>
                    <td>1343.7324</td>
                    <td>36.6570</td>
                    <td>0.9872</td>
                    <td>0.4437</td>
                </tr>
                <tr>
                    <td>With Battery ID Feature</td>
                    <td>6.6877</td>
                    <td>261.0975</td>
                    <td>15.6079</td>
                    <td>0.9975</td>
                    <td>0.0828</td>
                </tr>
            </tbody>
        </table>

        <div class="findings">
            <h4>Key Observations:</h4>
            <ul>
                <li>Outlier removal offered the most significant improvement, bringing the RMSE down by over 5 cycles but this is not useful because baseline has not been established yet, as model is unrealistically good and I still need to make it trustworthy.</li>
                <li>Adding battery_id as a feature increased accuracy but introduced the risk of leakage</li>
                <li>Prediction error is higher when RUL is low and decreases as the estimated RUL increases</li>
            </ul>
        </div>


        <p>While the Extra Trees Regressor (selected via PyCaret's compare_models()) achieved strong battery-wise generalization (RMSE=35.97, R²=0.9876), further validation is critical: time-based splits should test temporal robustness, and leave-multiple-battery-out evaluation could reveal dependency on specific units. Though performance aligns with literature (e.g., Zhang et al.'s physics-informed models ( <a href="https://doi.org/10.1016/j.jpowsour.2020.228710" target="_blank">Zhang et al., 2020</a>)), PyCaret's automated feature selection warrants inspection—key degradation features may dominate. Future work should compare against sequential models (e.g., LSTMs) and validate on heterogeneous battery datasets to assess industrial applicability.</p>


        <p>The consistent inverse relationship between prediction error and RUL—higher errors near end-of-life (EOL)—reflects fundamental degradation physics: early-stage capacity fade often follows quasi-linear trends (well-captured by PyCaret’s top-performing tree models), while EOL behavior becomes nonlinear due to cascading failures such as lithium plating and SEI layer breakdown (<a href="https://doi.org/10.1149/2.0411913jes" target="_blank">J. Electrochem. Soc. (2019)</a>). This pattern aligns with observations from the <a href="https://ti.arc.nasa.gov/tech/dash/groups/pcoe/" target="_blank">NASA PCoE Dataset</a>, where prediction uncertainty spikes below 50 cycles RUL. While PyCaret’s compare_models() efficiently surfaces this trend via aggregated metrics, targeted mitigation—such as EOL-focused ensemble weights or survival analysis techniques (<a href="https://doi.org/10.1016/j.est.2022.105346" target="_blank">Energy Storage (2022)</a>)—could further improve actionable warning times.</p>
    </div>

    <div class="version-section">
 <div class="version-title">Version 3: Finally Some Structure [ <a href=https://colab.research.google.com/drive/1KDEShRlfxGNq2d7QzcP6o7aIdSl30wnq#scrollTo=EQbaDkoo2SNr> Notebook</a> ]</div>
        
        <p>Version 3 represents a methodological maturation—moving from ad-hoc experimentation to rigorous ML engineering practices. The key insight: <strong>proper validation requires proper structure</strong>, both in data splitting and model evaluation.</p>

        <h4>The Structural Revolution</h4>
        
        <p>Building on Version 2's battery-wise splitting revelation, Version 3 introduces <strong>leave-multiple-batteries-out validation</strong> with systematic preprocessing pipelines. Instead of arbitrary single-battery holdouts, this version implements a more robust approach by leaving out batteries 13 and 14 for testing.</p>

        <p>More critically, Version 3 eliminates the battery_id leakage trap that plagued earlier versions. While Version 2 showed that including battery_id as a feature boosted performance (RMSE: 15.61), Version 3 properly excludes it from model training while preserving it for GroupKFold validation—critical for real-world deployment scenarios.</p>

        <table>
            <thead>
                <tr>
                    <th>Model</th>
                    <th>MAE</th>
                    <th>MSE</th>
                    <th>RMSE</th>
                    <th>R²</th>
                    <th>MAPE</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Extra Trees Regressor</td>
                    <td>58.82</td>
                    <td>5831.39</td>
                    <td>76.36</td>
                    <td>0.9439</td>
                    <td>0.4181</td>
                </tr>
            </tbody>
        </table>

        <div class="findings">
            <h4>The Leakage Lesson:</h4>
            <ul>
                <li>Version 1: RMSE ~19 (random split with temporal leakage)</li>
                <li>Version 2: RMSE ~36 (battery-wise split, but still some leakage)</li>
                <li>Version 3: RMSE ~76 (proper isolation, no feature leakage)</li>
            </ul>
        </div>

        <p>With target variance ~103,300, these results reflect <strong>genuine generalization capability</strong>. The 4x performance degradation from Version 1 isn't failure—it's <strong>methodological honesty</strong>. Real-world battery RUL prediction means deploying on completely unseen units, and Version 3's approach finally simulates this correctly.</p>

        <p>The MAPE of ~42% might seem high, but context is critical. Battery degradation exhibits high variability even within identical units due to manufacturing tolerances, thermal history, and usage patterns. Academic literature often reports overly optimistic results due to similar leakage issues that Version 3 explicitly addresses. The consistent R² of 0.94+ across proper validation suggests the model captures fundamental degradation physics, not just memorized patterns.</p>

        <p>The GroupKFold validation strategy ensures that during cross-validation, entire batteries are held out—preventing the model from learning battery-specific artifacts. This mirrors the final train-test split and provides more realistic performance estimates during hyperparameter tuning. The preprocessing pipeline's systematic approach—removing temporal features like Cycle_Index while preserving electrochemical signals—creates a foundation for genuine degradation modeling rather than time-series memorization.</p>

        <p>Version 3 establishes the methodological baseline for trustworthy battery RUL prediction. Future work should explore temporal validation with time-based splits, heterogeneous dataset validation across manufacturers, uncertainty quantification beyond point estimates, and sequential modeling with LSTM/Transformer architectures. The journey from Version 1's impressive-but-hollow metrics to Version 3's honest-but-actionable results exemplifies the difference between research theater and production readiness—sometimes, the best progress looks like taking a step backward.</p>
    </div>

<h2>Up Next</h2>

    <p>This week, I'll be tackling the <strong>End-of-Life (EOL) split challenge</strong>—a critical decision that could fundamentally reshape our modeling approach. The question: should we train separate models for different battery life stages, or maintain the unified approach that Version 3 established?</p>

    <p>The physics argues for splitting. Early-stage degradation follows quasi-linear capacity fade patterns that tree-based models handle well, while end-of-life behavior becomes chaotic—dominated by cascading failures like lithium plating and SEI breakdown. Our Version 2 observation that "prediction error spikes when RUL is low" suggests these aren't just different data points, but fundamentally different phenomena.</p>

    <p>Yet the pragmatics argue against it. EOL data is already scarce and high-variance. Further splitting risks creating unreliable test sets, and real-world deployment will encounter the full RUL spectrum. Where would we even draw the line—50 cycles? 100? The boundary becomes arbitrary.</p>

    <p>My current plan: implement a <strong>hybrid stratification approach</strong>. Keep the battery-wise split that Version 3 established, but analyze performance separately across RUL ranges. This preserves methodological rigor while revealing whether the model truly understands degradation physics or just averages across life stages.</p>

    <p>Beyond EOL splits, the broader modeling architecture question looms: traditional ML vs. deep learning? Interpretable tree ensembles vs. sequence-aware LSTMs? The answer will likely involve ensemble learning—because when predicting something as complex as battery failure, perhaps five models can collectively converge on truth where one cannot.</p>

    <p>Stay tuned for more updates. And if you've ever felt overwhelmed by a pile of half-working notebooks scattered across multiple "final" versions—rest assured, you're not alone.</p>

    <footer>
        <p>&copy; Fanny Fushayi 2025. All rights reserved.</p>
    </footer>
</body>
</html>